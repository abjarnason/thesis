\chapter{WebSocket Messaging System}
\label{chapter:websocketMessagingSystem}

This chapter provides information regarding the requirements and tooling for a WebSocket based messaging system. Afterwards the design and implementation of the system itself is introduced before benchmarking results and ideas for possible improvements are presented.

\section{Requirements}

The WebSocket messaging system allows a web chat client to communicate with a backend service using the WebSocket protocol rather than polling or long-polling. The system should be an improvement over HTTP-based implementations as it causes less network load and information can be updated instantly when available. The system must be capable of serving multiple concurrent clients while guaranteeing reliable message delivery.
\\ \\
The development of the WebSocket messaging system takes place while a HTTP-based implementation is in production. Thus it must be able to co-exist with the current implementation and be functional with the same backend setup. The components of the system should consist of modular and exchangeable microservices so that individual parts of the system can be scaled independently. Finally the system should be able to run on a Linux-based infrastructure.

\section{Tools}

The following tools were used to implement the WebSocket messaging system. They are by far the only tools available for the development of this kind of system but were deemed suitable based on accessibility, documentation and community backing.

\subsection{Node.js}

Node.js is an open source, cross-platform runtime environment for server-side and networking applications written in Javascript. It provides an event-driven architecture and a non-blocking I/O API optimized for throughput and scalability. Node.js operates on a single thread, making it possible to support multiple concurrent connections without incurring the cost of context-switching. Those properties make Node.js specially suitable for real-time and high concurrent applications. The WebSocket messaging system was implemented using JavaScript and two different Node.js versions were used to benchmark the system.

\subsection{ZeroMQ}

ZeroMQ is a messaging library for for distributed applications with an asynchronous I/O model. It offers the possibility of various messaging patterns such as request-reply, publish-subscribe and push-pull. One of the main advantage of ZeroMQ for application developers is that it manages various low-level networking considerations such as buffering and reconnection. That can lead to a more simple application development process and more robust distributed systems. ZeroMQ was used to send data between different components of the WebSocket messaging system using push sockets.

\subsection{Nginx}

Nginx is an open-source reverse proxy server with support for various protocols. It can also work as a load balancer, HTTP cache and web server. It uses an asynchronous event-driven approach for handling requests which make it capable of high concurrency and low memory footprint. Nginx was used as a reverse proxy server for the WebSocket messaging system that listens to ports 80 and 443 for HTTP and HTTPS traffic.

\section{Design and Implementation}

The WebSocket messaging system is designed to allow the clients and server to exchange data over a persistent WebSocket connection while data is served from the backend over a HTTP-based API. One challenge related to that kind of data exchange and the decision to use Node.js to implement the system is the single threaded nature of Node.js. To work around that aspect the the cluster module of Node.js will be used that makes it possible to take advantage of multi-core systems by making it possible to launch a cluster of processes. Another design decision is to keep the system in loosely coupled and fine grained components in accordance with the microservices ideology. This kind of architecture would make it possible to scale individual parts of the system for performance enhancement and distribute the system strategically, when put into production, to possibly reduce latency. Figure~\ref{fig:webSocketMessagingSystem} shows the structure of the WebSocket messaging system and the relationship between different components which are explained in the following sections.
\\
\begin{figure}[h!]
	\centering
	\label{fig:webSocketMessagingSystem}\includegraphics[width=1\textwidth]{images/websocketMessagingSystem}
	\caption{Structure of the WebSocket messaging system}
\end{figure}

\subsection{Clients}

A client opens up a connection with the WebSocket server and makes a request for specific keys. In the case of a chat application these keys would most likely be conversation identifiers that are kept in a persistent storage on the backend side. Different clients request different keys based on the conversations they are a part of and thus subscribed to. In the case of the WebSocket messaging system the client is simply a viewer for data randomly generated by the data provider. The system is designed to support multiple concurrent connections between clients and the WebSocket server.

\subsection{Reverse Proxy}

The reverse proxy is used as a tunnel between the clients to the WebSocket server and provides an encrypted connection with Transport Layer Security. The load balancing capabilities of Nginx make it possible to scale individual parts of the system. This approach makes it possible to scale the system horizontally when bottlenecks have been identified for better infrastructure utilization.


\subsection{WebSocket Server}

The WebSocket server listens on a specific port and establishes a WebSocket connection upon request. Further connection management is dependent on the WebSocket implementation used by the server. The Node.js WebSocket implementation used in the system is called ws\footnote{\url{https://github.com/websockets/ws}} and adds no extra functionality to the WebSocket API and protocol specifications. The aim of the project is to stay up to date with RFC 6455 while providing a simple and fast implementation of WebSocket.
\\ \\
After a WebSocket connection had been established the server is responsible for returning data associated with the key requested by a client. Either the server has the data in memory or it must request it from the fetcher using a push socket over a TCP connection using the ZeroMQ library. The communication with the fetcher goes through a ZeroMQ client is needed in able to use the binary protocol of the library.

\subsection{Fetcher}

The fetcher maintains a queue of requests made by the WebSocket server and manages a cluster of worker processes that handle the HTTP requests to the data provider. This technique makes it possible to have separate processes share the same server port and take an advantage of the available cores on the host machine. A master process listens on a port, accepts new connections and distributes them to available worker processes using a round-robin scheduling algorithm \cite{nodeCluster}. The worker processes are spawned such that they can communicate with the master process via IPC and are capable of passing server handles back and forth. The master process is then responsible for managing the worker process to avoid overloading and load-balance requests from the WebSocket server to different workers.
\\ \\
The clustering approach allows the WebSocket messaging system to improve throughput and resource utilization when managing production workload by taking advantage of multicore processors despite the single threaded nature of Node.js. Another advantage of this approach compared to communication patterns like polling and long-polling is that the fetcher and the data provider and be places geographically close to each other which can reduced latency.
\\
\begin{figure}[h!]
	\centering
	\label{fig:webSocketMessagingSystem}\includegraphics[width=0.9\textwidth]{images/poolOfWorkers}
	\caption{A Node.js cluster that routes requests to a pool of workers \cite{judd2008node}}
\end{figure}

\subsection{Data Provider}

The data provider serves JSON or random text and binary data over a HTTP-based API. In this implementation the data provider does not provide any meaningful data but has the purpose of simulating the data source that the WebSocket messaging system is intended to eventually use.

\section{Benchmarking}

The purpose of the benchmarking tests was to figure out how many concurrent clients the messaging system could handle without dropping connections. The results can be used as a basis for capacity planning of a WebSocket-based infrastructure or used to analyse the WebSocket messaging system for further development.
\\ \\
The load test was implemented so that a new client connects to the WebSocket server every two seconds and requests a 130 KB JSON file every second. The benchmarking was done for two different instance types, one with 4GB RAM  and 2 CPUs and another one with 8GB RAM and 4 CPUs. Both instances had SSD drives and were located in a datacenter in Frankfurt with the clients that made request located in Iceland. Version 0.10.38 of Node.js (the minor version provided in repositories of most common Linux distros) was used to run all the components of the system that were deployed on a Ubuntu 14.04.2 LTS OS with a 3.13.0-52 Linux kernel. The Nodetime\footnote{\url{https://nodetime.com/}} application monitoring and analytics service was for performance measurement and the \texttt{iptraf} Unix tool was used to measure network traffic.
\\ \\
The metrics of the benchmarking test were: Available memory, CPU time, V8 heap used and full garbage collections while the WebSocket messaging system was able to cope with increasing number of concurrent client connections. The duration of the tests was around ten minutes in both cases and the results can be seen in following figures.
\\
\begin{figure}[h!]
	\centering
	\subfigure[4GB RAM / 2 CPUs]{\label{fig:freeMemory-2cpus}\includegraphics[width=0.45\textwidth]{images/freeMemory-2cpus}} \hfill
	\subfigure[8GB RAM / 4 CPUs]{\label{fig:freeMemory-4cpus}\includegraphics[width=0.45\textwidth]{images/freeMemory-4cpus}}
	\caption{Free memory (MB)}
	\label{fig:freeMemory}
\end{figure}

\noindent
Figures~\ref{fig:freeMemory-2cpus} and~\ref{fig:cpuTime-4cpus} clearly show how the memory of the instances decreases as more clients connect to the WebSocket server until the server crashes and the instances regain some of their memory. The smaller instance managed 304 concurrent connections and the larger one managed 333 ones before the crash. Neither instance used any of its swap space so lack of physical memory was not the reason behind the connection saturation. This in an indicator that a horizontal scaling strategy might be more suitable for the messaging system than a vertical one.
\\
\begin{figure}[h!]
	\centering
	\subfigure[4GB RAM / 2 CPUs]{\label{fig:cpuTime-2cpus}\includegraphics[width=0.45\textwidth]{images/cpuTime-2cpus}} \hfill
	\subfigure[8GB RAM / 4 CPUs]{\label{fig:cpuTime-4cpus}\includegraphics[width=0.45\textwidth]{images/cpuTime-4cpus}}
	\caption{CPU time  (ms)}
\end{figure}

\noindent
As can be expected the time used for processing instructions by the CPUs increase incrementally with the number of concurrent connection as shown in figures~\ref{fig:cpuTime-2cpus} and~\ref{fig:cpuTime-4cpus}. Further analysis would be required to determine exactly how the CPU time is shared between different programs on the instances. That would be required to evaluate the benefit of the clustering implementation of the fetcher in messaging system. The true benefit of the worker pool used by the fetcher likely more related to the possibility to have concurrent network calls rather than parallel processing threads.
\\
\begin{figure}[h!]
	\centering
	\subfigure[4GB RAM / 2 CPUs]{\label{fig:v8heapUsed-2cpus}\includegraphics[width=0.45\textwidth]{images/v8heapUsed-2cpus}} \hfill
	\subfigure[8GB RAM / 4 CPUs]{\label{fig:v8heapUsed-4cpus}\includegraphics[width=0.45\textwidth]{images/v8heapUsed-4cpus}}
	\caption{V8 heap used (MB)}
\end{figure}

\noindent
Figures~\ref{fig:v8heapUsed-2cpus} and~\ref{fig:v8heapUsed-4cpus} show memory used for heap bookkeeping by the V8 JavaScript engine. It is hard to identify any special pattern in the figures as heap management is a low level operation with numerous different spaces so further analysis would be needed to truly understand what is going on. A possible approach for better heap management would be to use a newer version of Node.js. Version 0.10.38 (released in March 2015) uses a version of the V8 JavaScript engine that was released in October 2012\footnote{\url{https://code.google.com/p/v8/source/browse/trunk/ChangeLog}}. One of the main reasons behind io.js, the community driven fork of Node.js, was the slow adoption of newer versions of the V8 JavaScript engine. Future versions of Node.js should ship with a more up do date version of the V8 JavaScript engine as the two projects have been merged.
\\
\begin{figure}[h!]
	\centering
	\subfigure[4GB RAM / 2 CPUs]{\label{fig:fullGcs-2cpus}\includegraphics[width=0.45\textwidth]{images/fullGcs-2cpus}} \hfill
	\subfigure[8GB RAM / 4 CPUs]{\label{fig:fullGcs-4cpus}\includegraphics[width=0.45\textwidth]{images/fullGcs-4cpus}}
	\caption{Full garbage collections per minute}
\end{figure}

\noindent
As more clients connect to the WebSocket server the garbage collector kicks in more frequently which slows down overall performance because of increased memory usage. Figures~\ref{fig:fullGcs-2cpus} and~\ref{fig:fullGcs-4cpus} clearly show how the garbage collector tries to free up memory more frequently which eats up the CPU time as the load on the WebSocket server increases with more concurrent connections. When the garbage collector can not manage to free up sufficient memory the clients start to lose their connections. There does not seem to be much difference in the capability of each instance with regards to successful garbage collection which is another indicator that a that a horizontal scaling strategy might be more suitable for the messaging system than a vertical one.
\\
\begin{table}[h!]
\centering
\begin{tabular}{lll}
\hline
	& \textbf{4GB RAM/2 CPUs}	&	\textbf{8GB RAM/4 CPUs} \\ \hline
\begin{tabular}[v]{@{}c@{}}\textbf{Maximum concurrent}\\\textbf{clients (\#)}\end{tabular} & 304	& 333 \\
\begin{tabular}[v]{@{}c@{}}\textbf{Average outgoing}\\\textbf{rate (MB/sec)}\end{tabular} & 46,34	& 47,17 \\
\hline                         
\end{tabular}
\caption{Number of concurrent client connections and the rate of data transfer}
\label{table:connectionsDatarate}
\end{table}

\noindent
The maximum number of concurrent connections and average outgoing data rate during the load test is another indicator that the computational capabilities of the instances has a minimum effect on the performance of the WebSocket messaging system. The number of concurrent clients seems to be determined by the performance of the V8 JavaScript engine as seen before. The outgoing data rate can possibly be increased by configuring the network for increased bandwidth but that possibility depends on how the system is deployed. In the case when a public cloud service is used it can introduce a challenge as the control over the underlying infrastructure is often minimum. It is thus important to choose an infrastructure for deployment that allows enough control so that it can be tuned specially for this use case. The other option would be to adjust the WebSocket messaging system to the underlying infrastructure. In either case it is important to have a good understanding of how both the system and the environment where it is deployed. The best way to achieve that understanding is to monitor the system when it has been put into production use and make required configurations as they become evident.

\section{Possible Improvements and Further Work}

Improve the performance of the V8 engine. Do something to increase outgoing datarate

Use a different version of Node (a newer V8 engine might do something..)


